\documentclass{article}
\usepackage[margin=2cm]{geometry}

% for links
\usepackage{hyperref}

% to include images
\usepackage{graphicx}

% \begin{center}
% 	\includegraphics[clip,width=.8\textwidth]{figures/uml-use-case}
% \end{center}

% for equation environments
\usepackage{amsmath}

% for code snippets
\usepackage{listings}

% \begin{lstlisting}[language=Python]
% \end{lstlisting}

% \lstinputlisting[language=Octave]{BitXorMatrix.m}

% For header and footer
\usepackage{fancyhdr}
\usepackage{float} % formats figures
\pagestyle{fancy}
\fancyhf{} % clears default style
\lhead{F21BC Biologically Inspired Computation}
\lfoot{Kyle Mckay (km2008), Lina Rietuma (lr2004)}
\cfoot{\thepage}
\rfoot{HWU Person IDs: H00358352, H00361943}
\renewcommand{\headrulewidth}{0pt}

% No paragraph indentation
\setlength\parindent{0pt}
\setlength\parskip{1em}
\raggedright

\begin{document}

\title{Multi-layer Artificial Neural Network}

\begin{center}
  \Large{Multi-layer Artificial Neural Network Implementation}
\end{center}

\section{Introduction}

\section{Program development rationale}

The ANN was implemented in Python to make use of the NumPy library which provides convenient and fast matrix arithmetic implementations --
needed to efficiently perform forward and backward propagation.

The implementation was realised as a pair of classes: \lstinline{layer} and \lstinline{network}. This decision was made from the start because
it was noticed that propagation across the whole network (in either
direction) can be considered as propagation across a single layer at a time in sequence. In this way a generalisable implementation can be achieved to later allow very easy modification of hyperparameters.

Propagation itself is achieved through carefully considered matrix
operations such that the output of each layer can be fed
forwards/backwards directly to the next and the process repeated along
the sequence. Use of matrix operations enables efficient calculation
across all nodes, instances and features at once in each layer rather
than looping through each.

For convenience, the activation functions and loss function used by the network are configurable as parameters in the constructors of the layer and network respectively. These are realised as instances of the \lstinline{dfunc} class defined in funcs.py which just conveniently packages the function and its first derivative together for later use.

By default, cross entropy loss function is used, which outperforms quadratic cost function in classification problems and addresses learning slowdown when sigmoid values approach 0.\cite{Nielsen} \cite{Xavier} Weight initialisation is realised as Gaussian random variables - a form of weight normalisation by accounting for the number of input features (i.e., the number of nodes in the previous layer) and thus minimising the possibility of weight saturation - an issue encountered when random weights were initially used.\cite{Nielsen}

\section{Methods}

To train and test the model, UCI banknote authentication dataset is used.
Baseline architecture of 2 hidden layers and 4 nodes per layer is used, motivated by the initial informal investigation of hyper-parameters and their effect on the model's performance, with the specified architecture yielding the best results.
For the hidden layers, Leaky ReLU activation function is used to minimise the probability of gradient saturation and dying nodes., while for the output layer, sigmoid function is used to transform output into a suitable range (between 0 and 1) for error evaluation.

Throughout the experiments baseline hyper-parameters are used unless otherwise specified.

\subsection{Learning rate ($\alpha$)}

1. Finding $\alpha$ threshold value - during initial hyperparameter exploration, a large random learning rate was chosen (e.g., 2.5), and after running the baseline model for 10 repetitions the average rate of loss in the first ~30 epochs was observed. If the rate of loss had an upward trajectory, the initial value was reduced and the experiment repeated until the maximum value of $\alpha$ with a decreasing loss is obtained.
2. Using threshold $\alpha$ as the highest value, choose a range learning rates...

\subsection{Activation functions}

\subsection{Design hyperparameters}

 The number of nodes and the number of hidden layers (1, 2, 3, 4, 5 for layers and nodes, test all combos with reduced nr of epochs (25 combos in total) find which perform best, deep dive into those refer to \cite{Xavier} for why the number of hidden layers are limited to five. (possibly plot activation values per layer?)

\subsection{Input Normalisation}
Input normalisation - normal data vs feature standardised data (would have to be done when the train/ test data are there because we're trying to determine how well the model generalises based on the data it's been trained on).


\section{Results}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figs/layers.png}
  \caption{
    Average accuracy across 10 iterations for varied number of hidden
    layers.
  }
  \label{fig:layers}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figs/nodes.png}
  \caption{
    Average accuracy across 10 iterations for varied number of nodes
    in 2 hidden layers.
  }
  \label{fig:nodes}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figs/funcs.png}
  \caption{
    Average accuracy across 10 iterations for differing activation
    functions in 2 hidden layers each with 4 nodes and a learning rate
    \alpha=0.1 .
  }
  \label{fig:funcs}
\end{figure}

The number of hidden layers negatively affects a network's learning rate, hence the lower accuracies as the number of hidden layers increases. Surprisingly the model with 0 hidden layers (i.e., a single output node) outperforms the more complex architectures, especially on the test set (Figure 1). Inherently, any weight updates in a network with multiple layers will take more time to travel through the network and contribute to meaningful performance improvements, hence the decreased learning rate, nonetheless the model with the single node generalises unexpectedly well to unseen data which is likely a reflection of the simplified dataset used.

Conversely, the number of nodes has a reverse effect on the model's accuracy, increasing the number of  nodes improves the model's performance and ability to generalise to unseen data. Models with fewer nodes are more prone to overfit the data especially in the earlier epochs, characterised by the decreasing accuracy when using testing data (Figure 2).

The choice of activation function is critical to the model's performance, best demonstrated by the sigmoid model (Figure 3). In the 40 to 120 epoch range singmoid function gets 'stuck' and learning stalls, surprisingly the 40 epoch mark is also when the sigmoid model starts to generalise to the test dataset after severe overfitting up to that point. Nonetheless, by the end of  the 200 epochs, sigmoid model's accuracy is only slightly better than chance, very different from the 0.8 accuracy of the training dataset - a sign of overfitting and overall poor generalisation to unseen data.
Somewhat unexpected is the performance of the Tanh function which is comparable to and even surpasses the ReLU models in terms of accuracy, however this likely reflects the simplicity of this particular clasisfication problem. 

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figs/times.png}
  \caption{
    Average training time of each model across 10 iterations.
    Modified hyperparameter abbraviation identifies each model:
    L = hidden layers, N = nodes, S|T|R|LR = activation function,
    A = learning rate.
  }
  \label{fig:times}
\end{figure}

A linear relationship between average training time and the number of nodes per hidden layer, as well as the number of hidden layers is observed (Figure 4). Unexpected is the comparison between training times of models with different activation functions, with Tanh and Sigmoid outperforming the ReLU functions, since ReLU functions forgo the need for expensive exponent computations and divisions, designed to improve computation performance \cite{Niwa}. The results could be explained by the simplified dataset and network architecture used, with the performance benefits of ReLU and its variants becoming evident in more complex networks and classification problems.

Interestingly, training time is unaffected by the model's learning rate. When investigating alpha values further, dips in accuracy are observed, when using very small $\alpha$ values. Perhaps, due to the small gradient, the model gets stuck in a local minima, the minima is likely shallow which larger $\alpha$ values have no trouble escaping, hence the lack of dips in accuracy (see if this changes when the train/test sets are introduced (i.e., training set is reduced) which might have an effect on the landscape of the loss function \cite{Guo}). 

\section{Discussion and Conclusions}
Sigmoid and Tanh functions are sensitive to input in their mid-point range (where y = 0.5 and y =0.0 respectively), anything below or above theses values quickly reduces the learning rate (output value close to 0 or 1, any changes to weights have very minor impact) and eventually saturates the gradient.
Tanh output range of -1 to 1 has the advantage of adjusting weights in both positive or negative direction in a single iteration as opposed to sigmoid (justification for improved performance of tanh over sigmoid, reference?).
Rectified learning unit function forgoes learning slowdown encountered by sigmoid and tanh when very large inputs are used (ReLU has no upper limit), however negatively weighted inputs cause neurons to die off, stopping learning altogether. Leaky ReLU (an improved version of ReLU) addresses the issue of dying nodes by assigning negative inputs a small negative value (reference?)..

In retrospect, implementing adaptive learning rate and alternative weight initialisation approaches (e.g. Xavier) could have benefited to a more robust classifier.



\begin{thebibliography}{10}
\bibitem{Nielsen} Michael A. Nielsen, ``Neural Networks and Deep Learning", Determination Press, 2015
\bibitem{Xavier}Xavier Glorot, Yoshua Bengio, ``Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics", PMLR 9:249-256, 2010
\bibitem{Guo}  Yangzi Guo, Adrian Barbu, ``A study of local optima for learning feature interactions using neural networks", 2020
\bibitem{Niwa} Chigozie Nwankpa, Winifred Ijomah, Anthony Gachagan, & Stephen Marshall, ``Activation Functions: Comparison of trends in Practice and Research for Deep Learning", 2018

\end{thebibliography}

\end{document}