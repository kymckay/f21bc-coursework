\documentclass{article}
\usepackage[margin=2cm]{geometry}

% for links
\usepackage{hyperref}

% to include images
\usepackage{graphicx}

% \begin{center}
% 	\includegraphics[clip,width=.8\textwidth]{figures/uml-use-case}
% \end{center}

% for equation environments
\usepackage{amsmath}

% for code snippets
\usepackage{listings}

% \begin{lstlisting}[language=Python]
% \end{lstlisting}

% \lstinputlisting[language=Octave]{BitXorMatrix.m}

% For header and footer
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{} % clears default style
\lhead{F21BC Biologically Inspired Computation}
\lfoot{Kyle Mckay (km2008), Lina Rietuma (lr2004)}
\cfoot{\thepage}
\rfoot{HWU Person IDs: H00358352, H00361943}
\renewcommand{\headrulewidth}{0pt}

% No paragraph indentation
\setlength\parindent{0pt}
\setlength\parskip{1em}
\raggedright

\begin{document}

\title{Multi-layer Artificial Neural Network}

\begin{center}
  \Large{Multi-layer Artificial Neural Network Implementation}
\end{center}

\section{Introduction}

\section{Program development rationale}

The ANN was implemented in Python to make use of the NumPy library which provides convenient and fast matrix arithmetic implementations --
needed to efficiently perform forward and backward propagation.

The implementation was realised as a pair of classes: \lstinline{layer} and \lstinline{network}. This decision was made from the start because
it was noticed that propagation across the whole network (in either
direction) can be considered as propagation across a single layer at a time in sequence. In this way a generalisable implementation can be achieved to later allow very easy modification of hyperparameters.

Propagation itself is achieved through carefully considered matrix
operations such that the output of each layer can be fed
forwards/backwards directly to the next and the process repeated along
the sequence. Use of matrix operations enables efficient calculation
across all nodes, instances and features at once in each layer rather
than looping through each.

For convenience, the activation functions and loss function used by the network are configurable as parameters in the constructors of the layer and network respectively. These are realised as instances of the \lstinline{dfunc} class defined in funcs.py which just conveniently packages the function and its first derivative together for later use.

\section{Methods}

\section{Results}

\section{Discussion and Conclusions}

\section{References}

\end{document}