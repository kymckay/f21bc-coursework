\documentclass{article}
\usepackage[margin=2cm]{geometry}

% for links
\usepackage{hyperref}

% to include images
\usepackage{graphicx}

% \begin{center}
% 	\includegraphics[clip,width=.8\textwidth]{figures/uml-use-case}
% \end{center}

% for equation environments
\usepackage{amsmath}

% for code snippets
\usepackage{listings}

% \begin{lstlisting}[language=Python]
% \end{lstlisting}

% \lstinputlisting[language=Octave]{BitXorMatrix.m}

% For header and footer
\usepackage{fancyhdr}
\usepackage{float} % formats figures
\pagestyle{fancy}
\fancyhf{} % clears default style
\lhead{F21BC Biologically Inspired Computation}
\lfoot{Kyle Mckay (km2008), Lina Rietuma (lr2004)}
\cfoot{\thepage}
\rfoot{HWU Person IDs: H00358352, H00361943}
\renewcommand{\headrulewidth}{0pt}

% No paragraph indentation
\setlength\parindent{0pt}
\setlength\parskip{1em}
\raggedright

\begin{document}

\title{Multi-layer Artificial Neural Network}

\begin{center}
  \Large{Multi-layer Artificial Neural Network Implementation}
\end{center}

\section{Introduction}

\section{Program development rationale}

The ANN was implemented in Python to make use of the NumPy library which
provides convenient and fast matrix arithmetic implementations --
needed to efficiently perform forward and backward propagation.

The implementation was realised as a pair of classes: \lstinline{layer}
and \lstinline{network}. This decision was made from the start because
it was noticed that propagation across the whole network (in either
direction) can be considered as propagation across a single layer at a
time in sequence. In this way a generalisable implementation can be
achieved to later allow very easy modification of hyperparameters.

Propagation itself is achieved through carefully considered matrix
operations such that the output of each layer can be fed
forwards/backwards directly to the next and the process repeated along
the sequence. Use of matrix operations enables efficient calculation
across all nodes, instances and features at once in each layer rather
than looping through each.

For convenience, the activation functions and loss function used by the
network are configurable as parameters in the constructors of the layer
and network respectively. These are realised as instances of the
\lstinline{dfunc} class defined in funcs.py which just conveniently
packages the function and its first derivative together for later use.

By default, cross entropy loss function is used, which outperforms
quadratic cost function in classification problems and addresses
learning slowdown when sigmoid values approach 0 \cite{Nielsen}
\cite{Xavier}. Weight initialisation is realised as Gaussian random
variables - a form of weight normalisation by accounting for the number
of input features (i.e., the number of nodes in the previous layer) and
thus minimising the possibility of weight saturation -- an issue
encountered when random weights were initially used \cite{Nielsen}.

\section{Methods}

The ANN is trained and tested on the UCI banknote binary
classification dataset to evaluate how well it can perform and the
influence of hyperparameter selection.

% Tain test split reasoning (to see true performance for overfitting)
% Baseline hyperparameters set after intial informat exploration
% Hyperparameters and varied one at a time (citations)
% Results averaged over 10 iterations to account for random variance

\section{Results}


\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figs/layers.png}
  \caption{
    Average accuracy across 10 iterations for varied number of hidden
    layers.
  }
  \label{fig:layers}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figs/nodes.png}
  \caption{
    Average accuracy across 10 iterations for varied number of nodes
    in 2 hidden layers.
  }
  \label{fig:nodes}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figs/funcs.png}
  \caption{
    Average accuracy across 10 iterations for differing activation
    functions in 2 hidden layers each with 4 nodes and a learning rate
    \alpha=0.1 .
  }
  \label{fig:funcs}
\end{figure}

\subsection{Learning rate}

Interestingly, when using very small $\alpha$ values, dips in accuracy can be observed. Perhaps, due to the small gradient, the model gets stuck in a local minima, the minima is likely shallow which larger $\alpha$ values have no trouble escaping, hence the lack of dips in accuracy (see if this changes when the train/test sets are introduced (i.e., training set is reduced) which might have an effect on the landscape of the loss function \cite{Guo}).

To make any meaningful conclusion really need to observe how well the model generalises to a test data set.

\section{Discussion and Conclusions}
Sigmoid and Tanh functions are sensitive to input in their mid-point range (where y = 0.5 and y =0.0 respectively), anything below or above theses values quickly reduces the learning rate (output value close to 0 or 1, any changes to weights have very minor impact) and eventually saturates the gradient.
Tanh output range of -1 to 1 has the advantage of adjusting weights in both positive or negative direction in a single iteration as opposed to sigmoid (justification for improved performance of tanh over sigmoid, reference?).
Rectified learning unit function forgoes learning slowdown encountered by sigmoid and tanh when very large inputs are used (ReLU has no upper limit), however negatively weighted inputs cause neurons to die off, stopping learning altogether. Leaky ReLU (an improved version of ReLU) addresses the issue of dying nodes by assigning negative inputs a small negative value (reference?)..

Flatlining - the model gets stuck in a local optima (i.e. surrounded by steep loss function? consider the error landscape, might have saddle/ flat areas which are hard to escape).

In retrospect, implementing adaptive learning rate and alternative weight initialisation approaches (e.g. Xavier) could have benefited to a more robust classifier.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figs/times.png}
  \caption{
    Average training time of each model across 10 iterations.
    Modified hyperparameter abbraviation identifies each model:
    L = hidden layers, N = nodes, S|T|R|LR = activation function,
    A = learning rate.
  }
  \label{fig:times}
\end{figure}

\begin{thebibliography}{10}
\bibitem{Nielsen} Michael A. Nielsen, ``Neural Networks and Deep Learning", Determination Press, 2015
\bibitem{Xavier}Xavier Glorot, Yoshua Bengio, ``Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics", PMLR 9:249-256, 2010
\bibitem{Guo}  Yangzi Guo, Adrian Barbu, ``A study of local optima for learning feature interactions using neural networks", 2020

\end{thebibliography}

\end{document}